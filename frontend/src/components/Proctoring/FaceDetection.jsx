import { useState, useEffect, useRef, useCallback } from 'react';\nimport { toast } from 'react-toastify';\n\nconst FaceDetection = ({ isActive, onViolation, examId, studentId, securityLevel = 2 }) => {\n  const videoRef = useRef(null);\n  const canvasRef = useRef(null);\n  const animationRef = useRef(null);\n  const faceDetectorRef = useRef(null);\n  const eyeTrackingRef = useRef(null);\n  const violationCooldownRef = useRef(new Set());\n  \n  const [stream, setStream] = useState(null);\n  const [isInitialized, setIsInitialized] = useState(false);\n  const [faceData, setFaceData] = useState({\n    faceDetected: false,\n    faceCount: 0,\n    facePosition: { x: 0, y: 0, width: 0, height: 0 },\n    confidence: 0,\n    eyeData: {\n      leftEye: { x: 0, y: 0, isOpen: true },\n      rightEye: { x: 0, y: 0, isOpen: true },\n      gazeDirection: { x: 0, y: 0 },\n      isLookingAtScreen: true\n    },\n    headPose: {\n      yaw: 0, // Left-right rotation\n      pitch: 0, // Up-down rotation\n      roll: 0 // Tilt rotation\n    }\n  });\n  const [violations, setViolations] = useState([]);\n  const [cameraPermission, setCameraPermission] = useState('prompt');\n  \n  // Violation types for face detection\n  const VIOLATION_TYPES = {\n    NO_FACE: 'no_face_detected',\n    MULTIPLE_FACES: 'multiple_faces',\n    FACE_NOT_VISIBLE: 'face_not_visible',\n    LOOKING_AWAY: 'looking_away',\n    SUSPICIOUS_MOVEMENT: 'suspicious_movement',\n    IDENTITY_MISMATCH: 'identity_mismatch',\n    POOR_LIGHTING: 'poor_lighting',\n    CAMERA_COVERED: 'camera_covered',\n    EYES_CLOSED: 'eyes_closed_extended',\n    HEAD_TURNED: 'head_turned_away'\n  };\n\n  // Log violation with cooldown\n  const logViolation = useCallback((type, details = {}) => {\n    if (violationCooldownRef.current.has(type)) return;\n    \n    violationCooldownRef.current.add(type);\n    setTimeout(() => violationCooldownRef.current.delete(type), 3000);\n\n    const violation = {\n      id: `${type}-${Date.now()}`,\n      type,\n      timestamp: new Date().toISOString(),\n      examId,\n      studentId,\n      details,\n      severity: getViolationSeverity(type)\n    };\n\n    setViolations(prev => [...prev, violation]);\n    onViolation && onViolation(violation);\n\n    // Show notification based on severity\n    const message = getViolationMessage(type);\n    if (violation.severity === 'critical') {\n      toast.error(`üö® ${message}`);\n    } else if (violation.severity === 'major') {\n      toast.warn(`‚ö†Ô∏è ${message}`);\n    } else if (securityLevel >= 2) {\n      toast.info(`‚ÑπÔ∏è ${message}`);\n    }\n  }, [examId, studentId, onViolation, securityLevel]);\n\n  // Get violation severity\n  const getViolationSeverity = (type) => {\n    switch (type) {\n      case VIOLATION_TYPES.MULTIPLE_FACES:\n      case VIOLATION_TYPES.IDENTITY_MISMATCH:\n      case VIOLATION_TYPES.CAMERA_COVERED:\n        return 'critical';\n      case VIOLATION_TYPES.NO_FACE:\n      case VIOLATION_TYPES.FACE_NOT_VISIBLE:\n      case VIOLATION_TYPES.LOOKING_AWAY:\n        return 'major';\n      default:\n        return 'minor';\n    }\n  };\n\n  // Get violation message\n  const getViolationMessage = (type) => {\n    switch (type) {\n      case VIOLATION_TYPES.NO_FACE:\n        return 'Please ensure your face is visible to the camera.';\n      case VIOLATION_TYPES.MULTIPLE_FACES:\n        return 'Multiple faces detected! Only the exam taker should be visible.';\n      case VIOLATION_TYPES.FACE_NOT_VISIBLE:\n        return 'Face not clearly visible. Please adjust your camera.';\n      case VIOLATION_TYPES.LOOKING_AWAY:\n        return 'Please keep your eyes on the exam screen.';\n      case VIOLATION_TYPES.SUSPICIOUS_MOVEMENT:\n        return 'Unusual movement detected. Please remain still.';\n      case VIOLATION_TYPES.IDENTITY_MISMATCH:\n        return 'Identity verification failed. Please contact support.';\n      case VIOLATION_TYPES.POOR_LIGHTING:\n        return 'Lighting is too dark. Please improve lighting.';\n      case VIOLATION_TYPES.CAMERA_COVERED:\n        return 'Camera appears to be covered or blocked.';\n      case VIOLATION_TYPES.EYES_CLOSED:\n        return 'Eyes appear to be closed for extended period.';\n      case VIOLATION_TYPES.HEAD_TURNED:\n        return 'Please keep your head facing forward.';\n      default:\n        return 'Face detection violation.';\n    }\n  };\n\n  // Initialize camera\n  const initializeCamera = useCallback(async () => {\n    try {\n      const mediaStream = await navigator.mediaDevices.getUserMedia({\n        video: {\n          width: { ideal: 1280 },\n          height: { ideal: 720 },\n          frameRate: { ideal: 30 },\n          facingMode: 'user'\n        },\n        audio: false\n      });\n      \n      setStream(mediaStream);\n      setCameraPermission('granted');\n      \n      if (videoRef.current) {\n        videoRef.current.srcObject = mediaStream;\n        videoRef.current.onloadedmetadata = () => {\n          videoRef.current.play();\n          initializeFaceDetection();\n        };\n      }\n    } catch (error) {\n      console.error('Camera initialization failed:', error);\n      setCameraPermission('denied');\n      toast.error('Camera access required for exam proctoring.');\n      \n      logViolation(VIOLATION_TYPES.CAMERA_COVERED, {\n        error: error.message,\n        name: error.name\n      });\n    }\n  }, [logViolation]);\n\n  // Initialize face detection using MediaPipe or fallback to basic detection\n  const initializeFaceDetection = useCallback(async () => {\n    try {\n      // Try to use MediaPipe FaceDetection if available\n      if (window.FaceDetection) {\n        const faceDetection = new window.FaceDetection({\n          model: 'short',\n          minDetectionConfidence: 0.5\n        });\n        await faceDetection.initialize();\n        faceDetectorRef.current = faceDetection;\n      } \n      // Fallback to basic face detection using getUserMedia constraints\n      else if ('FaceDetector' in window) {\n        faceDetectorRef.current = new window.FaceDetector({\n          maxDetectedFaces: 5,\n          fastMode: false\n        });\n      }\n      \n      setIsInitialized(true);\n      startDetection();\n    } catch (error) {\n      console.error('Face detection initialization failed:', error);\n      // Continue with basic monitoring even without face detection\n      setIsInitialized(true);\n      startDetection();\n    }\n  }, []);\n\n  // Start detection loop\n  const startDetection = useCallback(() => {\n    const detect = async () => {\n      if (!isActive || !videoRef.current || videoRef.current.paused) {\n        animationRef.current = requestAnimationFrame(detect);\n        return;\n      }\n\n      try {\n        await performFaceDetection();\n        analyzeGazeDirection();\n        checkViolations();\n      } catch (error) {\n        console.error('Detection error:', error);\n      }\n\n      animationRef.current = requestAnimationFrame(detect);\n    };\n\n    detect();\n  }, [isActive]);\n\n  // Perform face detection\n  const performFaceDetection = useCallback(async () => {\n    if (!videoRef.current || !canvasRef.current) return;\n\n    const video = videoRef.current;\n    const canvas = canvasRef.current;\n    const ctx = canvas.getContext('2d');\n    \n    // Set canvas size to match video\n    canvas.width = video.videoWidth;\n    canvas.height = video.videoHeight;\n    \n    // Draw current frame\n    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n\n    let detectedFaces = [];\n    \n    try {\n      // Use MediaPipe if available\n      if (faceDetectorRef.current && faceDetectorRef.current.detectFaces) {\n        detectedFaces = await faceDetectorRef.current.detectFaces(canvas);\n      }\n      // Use FaceDetector API if available\n      else if (faceDetectorRef.current && faceDetectorRef.current.detect) {\n        detectedFaces = await faceDetectorRef.current.detect(canvas);\n      }\n      // Basic fallback detection using image analysis\n      else {\n        detectedFaces = await basicFaceDetection(ctx, canvas.width, canvas.height);\n      }\n    } catch (error) {\n      console.error('Face detection error:', error);\n      detectedFaces = [];\n    }\n\n    // Update face data\n    const newFaceData = {\n      faceDetected: detectedFaces.length > 0,\n      faceCount: detectedFaces.length,\n      facePosition: detectedFaces[0] ? extractFacePosition(detectedFaces[0]) : { x: 0, y: 0, width: 0, height: 0 },\n      confidence: detectedFaces[0] ? (detectedFaces[0].confidence || 0.8) : 0,\n      eyeData: detectedFaces[0] ? extractEyeData(detectedFaces[0]) : faceData.eyeData,\n      headPose: detectedFaces[0] ? extractHeadPose(detectedFaces[0]) : faceData.headPose\n    };\n\n    setFaceData(newFaceData);\n    \n    // Draw detection results on canvas if in debug mode\n    if (process.env.NODE_ENV === 'development') {\n      drawDetectionResults(ctx, detectedFaces);\n    }\n  }, [faceData.eyeData, faceData.headPose]);\n\n  // Basic face detection fallback using color analysis\n  const basicFaceDetection = useCallback(async (ctx, width, height) => {\n    // This is a simplified face detection based on skin color detection\n    const imageData = ctx.getImageData(0, 0, width, height);\n    const data = imageData.data;\n    \n    let skinPixels = 0;\n    let totalPixels = data.length / 4;\n    \n    // Simple skin color detection\n    for (let i = 0; i < data.length; i += 4) {\n      const r = data[i];\n      const g = data[i + 1];\n      const b = data[i + 2];\n      \n      // Basic skin color range\n      if (r > 95 && g > 40 && b > 20 && \n          Math.max(r, g, b) - Math.min(r, g, b) > 15 &&\n          Math.abs(r - g) > 15 && r > g && r > b) {\n        skinPixels++;\n      }\n    }\n    \n    const skinRatio = skinPixels / totalPixels;\n    \n    // If significant skin color detected, assume face present\n    if (skinRatio > 0.02) {\n      return [{\n        confidence: Math.min(skinRatio * 10, 1),\n        boundingBox: {\n          x: width * 0.3,\n          y: height * 0.2,\n          width: width * 0.4,\n          height: height * 0.5\n        }\n      }];\n    }\n    \n    return [];\n  }, []);\n\n  // Extract face position from detection result\n  const extractFacePosition = (face) => {\n    if (face.boundingBox) {\n      return {\n        x: face.boundingBox.x,\n        y: face.boundingBox.y,\n        width: face.boundingBox.width,\n        height: face.boundingBox.height\n      };\n    }\n    return { x: 0, y: 0, width: 0, height: 0 };\n  };\n\n  // Extract eye data from face landmarks\n  const extractEyeData = (face) => {\n    // This would be implemented with actual landmark data from MediaPipe\n    if (face.landmarks) {\n      // Extract eye landmarks and calculate eye positions, gaze direction, etc.\n      const leftEye = face.landmarks.leftEye || { x: 0, y: 0 };\n      const rightEye = face.landmarks.rightEye || { x: 0, y: 0 };\n      \n      return {\n        leftEye: { ...leftEye, isOpen: true },\n        rightEye: { ...rightEye, isOpen: true },\n        gazeDirection: calculateGazeDirection(leftEye, rightEye),\n        isLookingAtScreen: checkIfLookingAtScreen(leftEye, rightEye)\n      };\n    }\n    \n    return faceData.eyeData;\n  };\n\n  // Extract head pose from face data\n  const extractHeadPose = (face) => {\n    if (face.landmarks) {\n      // Calculate head pose angles from landmarks\n      return {\n        yaw: face.yaw || 0,\n        pitch: face.pitch || 0,\n        roll: face.roll || 0\n      };\n    }\n    \n    return faceData.headPose;\n  };\n\n  // Calculate gaze direction\n  const calculateGazeDirection = (leftEye, rightEye) => {\n    const centerX = (leftEye.x + rightEye.x) / 2;\n    const centerY = (leftEye.y + rightEye.y) / 2;\n    \n    // Normalize to screen coordinates\n    return {\n      x: (centerX - 0.5) * 2, // -1 to 1\n      y: (centerY - 0.5) * 2  // -1 to 1\n    };\n  };\n\n  // Check if looking at screen\n  const checkIfLookingAtScreen = (leftEye, rightEye) => {\n    const gazeDirection = calculateGazeDirection(leftEye, rightEye);\n    \n    // Consider looking at screen if gaze is within reasonable bounds\n    return Math.abs(gazeDirection.x) < 0.3 && Math.abs(gazeDirection.y) < 0.3;\n  };\n\n  // Analyze gaze direction\n  const analyzeGazeDirection = useCallback(() => {\n    if (securityLevel < 2) return;\n    \n    const { eyeData } = faceData;\n    \n    // Check if looking away from screen\n    if (!eyeData.isLookingAtScreen && faceData.faceDetected) {\n      logViolation(VIOLATION_TYPES.LOOKING_AWAY, {\n        gazeDirection: eyeData.gazeDirection,\n        timestamp: Date.now()\n      });\n    }\n  }, [faceData, logViolation, securityLevel]);\n\n  // Check for various violations\n  const checkViolations = useCallback(() => {\n    const { faceDetected, faceCount, confidence, headPose } = faceData;\n    \n    // No face detected\n    if (!faceDetected) {\n      logViolation(VIOLATION_TYPES.NO_FACE, {\n        timestamp: Date.now(),\n        frameAnalyzed: true\n      });\n      return;\n    }\n    \n    // Multiple faces detected\n    if (faceCount > 1) {\n      logViolation(VIOLATION_TYPES.MULTIPLE_FACES, {\n        faceCount,\n        timestamp: Date.now()\n      });\n    }\n    \n    // Low confidence (poor visibility)\n    if (confidence < 0.3) {\n      logViolation(VIOLATION_TYPES.FACE_NOT_VISIBLE, {\n        confidence,\n        reason: 'low_confidence'\n      });\n    }\n    \n    // Head turned away significantly\n    if (securityLevel >= 3 && Math.abs(headPose.yaw) > 30) {\n      logViolation(VIOLATION_TYPES.HEAD_TURNED, {\n        headPose,\n        angle: headPose.yaw\n      });\n    }\n  }, [faceData, logViolation, securityLevel]);\n\n  // Draw detection results (for debugging)\n  const drawDetectionResults = (ctx, faces) => {\n    ctx.strokeStyle = '#00ff00';\n    ctx.lineWidth = 2;\n    \n    faces.forEach(face => {\n      if (face.boundingBox) {\n        const { x, y, width, height } = face.boundingBox;\n        ctx.strokeRect(x, y, width, height);\n        \n        // Draw confidence\n        ctx.fillStyle = '#00ff00';\n        ctx.font = '16px Arial';\n        ctx.fillText(`${Math.round(face.confidence * 100)}%`, x, y - 5);\n      }\n    });\n  };\n\n  // Initialize when component becomes active\n  useEffect(() => {\n    if (isActive && !isInitialized) {\n      initializeCamera();\n    }\n    \n    return () => {\n      if (animationRef.current) {\n        cancelAnimationFrame(animationRef.current);\n      }\n    };\n  }, [isActive, isInitialized, initializeCamera]);\n\n  // Cleanup on unmount\n  useEffect(() => {\n    return () => {\n      if (stream) {\n        stream.getTracks().forEach(track => track.stop());\n      }\n      if (animationRef.current) {\n        cancelAnimationFrame(animationRef.current);\n      }\n    };\n  }, [stream]);\n\n  if (!isActive) {\n    return null;\n  }\n\n  return (\n    <div className=\"face-detection-container\">\n      {/* Hidden video element for processing */}\n      <video\n        ref={videoRef}\n        className=\"face-detection-video\"\n        autoPlay\n        muted\n        playsInline\n        style={{ display: 'none' }}\n      />\n      \n      {/* Hidden canvas for processing */}\n      <canvas\n        ref={canvasRef}\n        className=\"face-detection-canvas\"\n        style={{ display: process.env.NODE_ENV === 'development' ? 'block' : 'none' }}\n      />\n      \n      {/* Camera permission prompt */}\n      {cameraPermission === 'prompt' && (\n        <div className=\"camera-permission-prompt\">\n          <div className=\"permission-content\">\n            <h3>üìπ Camera Access Required</h3>\n            <p>This exam requires camera access for identity verification and monitoring.</p>\n            <button className=\"permission-btn\" onClick={initializeCamera}>\n              Grant Camera Access\n            </button>\n          </div>\n        </div>\n      )}\n      \n      {/* Camera denied message */}\n      {cameraPermission === 'denied' && (\n        <div className=\"camera-denied\">\n          <div className=\"denied-content\">\n            <h3>‚ùå Camera Access Denied</h3>\n            <p>Camera access is required to proceed with the exam. Please enable camera access and refresh the page.</p>\n            <button className=\"retry-btn\" onClick={initializeCamera}>\n              Retry Camera Access\n            </button>\n          </div>\n        </div>\n      )}\n      \n      {/* Face detection status */}\n      {isInitialized && cameraPermission === 'granted' && (\n        <div className=\"face-status\">\n          <div className={`face-indicator ${faceData.faceDetected ? 'detected' : 'not-detected'}`}>\n            <span className=\"face-icon\">\n              {faceData.faceDetected ? 'üòä' : '‚ùì'}\n            </span>\n            <div className=\"face-info\">\n              <div className=\"face-count\">\n                Faces: {faceData.faceCount}\n              </div>\n              {faceData.confidence > 0 && (\n                <div className=\"confidence\">\n                  Confidence: {Math.round(faceData.confidence * 100)}%\n                </div>\n              )}\n            </div>\n          </div>\n          \n          {/* Gaze indicator */}\n          {securityLevel >= 2 && faceData.eyeData.isLookingAtScreen !== undefined && (\n            <div className={`gaze-indicator ${faceData.eyeData.isLookingAtScreen ? 'looking' : 'away'}`}>\n              <span className=\"gaze-icon\">\n                {faceData.eyeData.isLookingAtScreen ? 'üëÄ' : 'üëÅÔ∏è'}\n              </span>\n              <div className=\"gaze-status\">\n                {faceData.eyeData.isLookingAtScreen ? 'Focused' : 'Looking Away'}\n              </div>\n            </div>\n          )}\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default FaceDetection;